{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_dct as dct\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(['science','no-latex', 'notebook'])\n",
    "\n",
    "from multiprocessing import Pool, Queue, Process, set_start_method\n",
    "import multiprocessing as mp_\n",
    "\n",
    "import time\n",
    "import pkbar\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from common import *\n",
    "from transform_based_network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T_Layer(nn.Module):\n",
    "    def __init__(self, dct_w, dct_b):\n",
    "        super(T_Layer, self).__init__()\n",
    "        self.weights = nn.Parameter(dct_w)\n",
    "        self.bias = nn.Parameter(dct_b)\n",
    "        \n",
    "    def forward(self, dct_x):\n",
    "        x = torch.mm(self.weights, dct_x) + self.bias\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Frontal_Slice(nn.Module):\n",
    "    def __init__(self, dct_w, dct_b):\n",
    "        super(Frontal_Slice, self).__init__()\n",
    "        self.device = dct_w.device\n",
    "        self.dct_linear = T_Layer(dct_w, dct_b)\n",
    "        self.linear1 = nn.Linear(28, 28)\n",
    "        self.linear2 = nn.Linear(28, 28)\n",
    "        self.classifier = nn.Linear(28, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.transpose(x, 0, 1).to(self.device)\n",
    "        x = self.dct_linear(x)\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.linear2(x)\n",
    "        x = self.classifier(x)\n",
    "        x = torch.transpose(x, 0, 1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(self, shape, device='cpu'):\n",
    "        super(Ensemble, self).__init__()\n",
    "        self.device = device    \n",
    "        self.models = []\n",
    "        for i in range(shape[0]):\n",
    "            dct_w, dct_b = make_weights(shape, device, scale=0.01)\n",
    "            model = Frontal_Slice(dct_w[i, ...], dct_b[i, ...])\n",
    "            self.models.append(model.to(device))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = torch.empty(x.shape[0], 10, x.shape[2])\n",
    "        dct_x = torch_apply(dct.dct, x).to(self.device)\n",
    "        for i in range(len(self.models)):\n",
    "            result[i, ...] = self.models[i](dct_x[i, ...])\n",
    "        result = torch_apply(dct.idct, result)\n",
    "        softmax = scalar_tubal_func(result)\n",
    "        return torch.transpose(softmax, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_ensemble(x, y, i=50, device='cuda:0'):\n",
    "    x = torch_shift(x).to(device)\n",
    "    y = y.to(device)\n",
    "    ensemble = Ensemble(x.shape, device).to(device)\n",
    "    params = []\n",
    "    for m in ensemble.models:\n",
    "        params += list(m.parameters())\n",
    "    optimizer = optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pbar = pkbar.Pbar(name='progress', target=i)\n",
    "    for j in range(i):\n",
    "        outputs = ensemble(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.to(device), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.update(j)\n",
    "        \n",
    "    print(loss.item())\n",
    "    return ensemble\n",
    "\n",
    "## 16, 10, 10, 100 iterations\n",
    "# cpu, for loop: 4.1s\n",
    "# gpu, for loop: 5.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.258345127105713\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2054741382598877\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2106308937072754\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3958640098571777\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.396796226501465\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3846657276153564\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.377892255783081\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.333148956298828\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.382307529449463\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.353180170059204\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.226315975189209\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.43098783493042\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.294565200805664\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2969651222229004\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.318237543106079\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.177316904067993\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3052825927734375\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.335710287094116\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.397152900695801\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.370769739151001\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3892109394073486\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.33648681640625\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.190135955810547\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.242469072341919\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3275198936462402\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2853877544403076\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.371673583984375\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4294893741607666\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.392704963684082\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.304870367050171\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.334470510482788\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2950997352600098\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.34368896484375\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3712351322174072\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.276841640472412\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4065818786621094\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.5686302185058594\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.308123826980591\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4058146476745605\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3507513999938965\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2929067611694336\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4763824939727783\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.282348394393921\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.386868953704834\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.487302303314209\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.293956756591797\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.346597194671631\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.355623722076416\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.425053119659424\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.316558361053467\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.328648805618286\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2996323108673096\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3672714233398438\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3782005310058594\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3096680641174316\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.279012680053711\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.385782480239868\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3129913806915283\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4803237915039062\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2476930618286133\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2953734397888184\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2799437046051025\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3046202659606934\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3246138095855713\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.341085433959961\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3427202701568604\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4116597175598145\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3802366256713867\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.192852258682251\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2277672290802\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3672399520874023\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4218380451202393\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3601598739624023\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2413578033447266\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.311567783355713\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.301414728164673\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4172964096069336\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3678677082061768\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2424652576446533\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3546462059020996\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.414595127105713\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.5252790451049805\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.321140766143799\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3022618293762207\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.372028112411499\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2597036361694336\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3568220138549805\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3252947330474854\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2794265747070312\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2034571170806885\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.336716651916504\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2175707817077637\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.300899028778076\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.188093662261963\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2138071060180664\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3868041038513184\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3908541202545166\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.3672962188720703\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.4023756980895996\n",
      "progress\n",
      "1/1  [==============================] - 0.1s\n",
      "2.2920897006988525\n"
     ]
    }
   ],
   "source": [
    "x0 = []\n",
    "y0 = []\n",
    "for i in range(100):\n",
    "    x0.append(torch.randn(16, 29, 28))\n",
    "    y0.append(torch.randint(10, (16,)))\n",
    "\n",
    "for i in range(100):\n",
    "    train_ensemble(x0[i], y0[i], i=1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d768e891d9ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_ensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "train_ensemble(x, y, i=20, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_slice(model, x_i, y_i):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.5, momentum=0.9, weight_decay=5e-4)\n",
    "    outputs = model(x_i, 2)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs.cuda(), y_i.cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data..\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "trainloader, testloader = load_mnist_multiprocess(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0\n",
      "tensor(2.9926, grad_fn=<NllLossBackward>)\n",
      "   1/1200  [..............................] - 0.2stensor(3.1387, grad_fn=<NllLossBackward>)\n",
      "   2/1200  [..............................] - 0.5stensor(2.7687, grad_fn=<NllLossBackward>)\n",
      "   3/1200  [..............................] - 0.8stensor(3.5608, grad_fn=<NllLossBackward>)\n",
      "   4/1200  [..............................] - 1.0stensor(3.6441, grad_fn=<NllLossBackward>)\n",
      "   5/1200  [..............................] - 1.3stensor(2.9113, grad_fn=<NllLossBackward>)\n",
      "   6/1200  [..............................] - 1.6stensor(2.9739, grad_fn=<NllLossBackward>)\n",
      "   7/1200  [..............................] - 1.8stensor(3.0492, grad_fn=<NllLossBackward>)\n",
      "   8/1200  [..............................] - 2.1stensor(3.1557, grad_fn=<NllLossBackward>)\n",
      "   9/1200  [..............................] - 2.4stensor(3.1349, grad_fn=<NllLossBackward>)\n",
      "  10/1200  [..............................] - 2.6stensor(3.6500, grad_fn=<NllLossBackward>)\n",
      "  11/1200  [..............................] - 2.9stensor(3.1542, grad_fn=<NllLossBackward>)\n",
      "  12/1200  [..............................] - 3.2stensor(3.4596, grad_fn=<NllLossBackward>)\n",
      "  13/1200  [..............................] - 3.4stensor(3.1558, grad_fn=<NllLossBackward>)\n",
      "  14/1200  [..............................] - 3.7stensor(3.7230, grad_fn=<NllLossBackward>)\n",
      "  15/1200  [..............................] - 4.0stensor(2.6558, grad_fn=<NllLossBackward>)\n",
      "  16/1200  [..............................] - 4.2stensor(2.7423, grad_fn=<NllLossBackward>)\n",
      "  17/1200  [..............................] - 4.5stensor(2.8582, grad_fn=<NllLossBackward>)\n",
      "  18/1200  [..............................] - 4.8stensor(2.9167, grad_fn=<NllLossBackward>)\n",
      "  19/1200  [..............................] - 5.1stensor(3.0637, grad_fn=<NllLossBackward>)\n",
      "  20/1200  [..............................] - 5.3stensor(3.1373, grad_fn=<NllLossBackward>)\n",
      "  21/1200  [..............................] - 5.6stensor(3.0705, grad_fn=<NllLossBackward>)\n",
      "  22/1200  [..............................] - 5.9stensor(3.5674, grad_fn=<NllLossBackward>)\n",
      "  23/1200  [..............................] - 6.1stensor(2.6606, grad_fn=<NllLossBackward>)\n",
      "  24/1200  [..............................] - 6.4stensor(3.5240, grad_fn=<NllLossBackward>)\n",
      "  25/1200  [..............................] - 6.7stensor(2.8672, grad_fn=<NllLossBackward>)\n",
      "  26/1200  [..............................] - 6.9stensor(3.2020, grad_fn=<NllLossBackward>)\n",
      "  27/1200  [..............................] - 7.2stensor(3.2500, grad_fn=<NllLossBackward>)\n",
      "  28/1200  [..............................] - 7.5stensor(2.8160, grad_fn=<NllLossBackward>)\n",
      "  29/1200  [..............................] - 7.7stensor(2.6766, grad_fn=<NllLossBackward>)\n",
      "  30/1200  [..............................] - 8.0stensor(3.2772, grad_fn=<NllLossBackward>)\n",
      "  31/1200  [..............................] - 8.3stensor(2.7980, grad_fn=<NllLossBackward>)\n",
      "  32/1200  [..............................] - 8.5stensor(2.6255, grad_fn=<NllLossBackward>)\n",
      "  33/1200  [..............................] - 8.8stensor(3.0841, grad_fn=<NllLossBackward>)\n",
      "  34/1200  [..............................] - 9.1stensor(3.2286, grad_fn=<NllLossBackward>)\n",
      "  35/1200  [..............................] - 9.3stensor(3.5063, grad_fn=<NllLossBackward>)\n",
      "  36/1200  [..............................] - 9.6stensor(3.6534, grad_fn=<NllLossBackward>)\n",
      "  37/1200  [..............................] - 9.9stensor(2.8134, grad_fn=<NllLossBackward>)\n",
      "  38/1200  [..............................] - 10.1stensor(3.6882, grad_fn=<NllLossBackward>)\n",
      "  39/1200  [..............................] - 10.4stensor(3.6806, grad_fn=<NllLossBackward>)\n",
      "  40/1200  [>.............................] - 10.7stensor(2.6873, grad_fn=<NllLossBackward>)\n",
      "  41/1200  [>.............................] - 10.9stensor(3.0474, grad_fn=<NllLossBackward>)\n",
      "  42/1200  [>.............................] - 11.2stensor(3.5188, grad_fn=<NllLossBackward>)\n",
      "  43/1200  [>.............................] - 11.5stensor(2.5640, grad_fn=<NllLossBackward>)\n",
      "  44/1200  [>.............................] - 11.7stensor(3.5495, grad_fn=<NllLossBackward>)\n",
      "  45/1200  [>.............................] - 12.0stensor(3.0442, grad_fn=<NllLossBackward>)\n",
      "  46/1200  [>.............................] - 12.3stensor(3.0024, grad_fn=<NllLossBackward>)\n",
      "  47/1200  [>.............................] - 12.5stensor(2.3992, grad_fn=<NllLossBackward>)\n",
      "  48/1200  [>.............................] - 12.8stensor(3.3050, grad_fn=<NllLossBackward>)\n",
      "  49/1200  [>.............................] - 13.1stensor(3.0773, grad_fn=<NllLossBackward>)\n",
      "  50/1200  [>.............................] - 13.4stensor(3.1223, grad_fn=<NllLossBackward>)\n",
      "  51/1200  [>.............................] - 13.6stensor(2.8709, grad_fn=<NllLossBackward>)\n",
      "  52/1200  [>.............................] - 13.9stensor(3.2700, grad_fn=<NllLossBackward>)\n",
      "  53/1200  [>.............................] - 14.2stensor(2.9188, grad_fn=<NllLossBackward>)\n",
      "  54/1200  [>.............................] - 14.4stensor(2.9130, grad_fn=<NllLossBackward>)\n",
      "  55/1200  [>.............................] - 14.7stensor(2.7989, grad_fn=<NllLossBackward>)\n",
      "  56/1200  [>.............................] - 15.0stensor(3.4453, grad_fn=<NllLossBackward>)\n",
      "  57/1200  [>.............................] - 15.3stensor(3.3369, grad_fn=<NllLossBackward>)\n",
      "  58/1200  [>.............................] - 15.5stensor(2.6348, grad_fn=<NllLossBackward>)\n",
      "  59/1200  [>.............................] - 15.8stensor(3.6960, grad_fn=<NllLossBackward>)\n",
      "  60/1200  [>.............................] - 16.0stensor(3.1618, grad_fn=<NllLossBackward>)\n",
      "  61/1200  [>.............................] - 16.3stensor(3.5006, grad_fn=<NllLossBackward>)\n",
      "  62/1200  [>.............................] - 16.6stensor(3.2523, grad_fn=<NllLossBackward>)\n",
      "  63/1200  [>.............................] - 16.8stensor(3.1775, grad_fn=<NllLossBackward>)\n",
      "  64/1200  [>.............................] - 17.1s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c4bc2a3c9efc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch_shift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mensemble\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnsemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7c0086969f6d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, shape, device)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mdct_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdct_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrontal_Slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdct_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\workstation\\TNN\\transform_based_network\\util.py\u001b[0m in \u001b[0;36mmake_weights\u001b[1;34m(shape, device, scale)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mdct_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mdct_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "for epoch in range(10):\n",
    "    pbar = pkbar.Pbar(name='Epoch'+str(epoch), target=60000/batch_size)\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        '''\n",
    "\n",
    "        dct_x = torch_apply(dct.dct, x.squeeze())\n",
    "        y_cat = to_categorical(y, 10) \n",
    "\n",
    "        dct_y_cat = torch.randn(y_cat.shape[0], dct_x.shape[1], 10)\n",
    "        for i in range(10):\n",
    "            dct_y_cat[:, i, :] = y_cat\n",
    "        dct_y_cat = torch_apply(dct.dct, dct_y_cat)\n",
    "        dct_x.to(device)\n",
    "        dct_y_cat.to(device)\n",
    "        '''\n",
    "\n",
    "        correct = 0\n",
    "        train_loss = 0\n",
    "        total = 0\n",
    "        inputs = torch_shift(inputs).to(device)\n",
    "        ensemble = Ensemble(inputs.shape, device).to(device)\n",
    "        params = []\n",
    "        \n",
    "        for m in ensemble.models:\n",
    "            params += list(m.parameters())\n",
    "        optimizer = optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        outputs = ensemble(inputs) \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.to(device), targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        total += batch_size\n",
    "        print(loss)\n",
    "        \n",
    "        pbar.update(batch_idx)\n",
    "    print(correct/total, train_loss/total)\n",
    "    \n",
    "\n",
    "'''\n",
    "    models = []\n",
    "    for i in range(16):\n",
    "        dct_w, dct_b = make_weights(dct_x.shape, device=device)\n",
    "        model = Frontal_Slice(dct_w[i, ...], dct_b[i, ...])\n",
    "        models.append(model.to(device))\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        train_slice(models[i], dct_x[i, ...], dct_y_cat[i, ...])\n",
    "    print()\n",
    "    pbar.update(batch_idx)\n",
    "    \n",
    "    tmp = torch_mp.get_context('spawn')\n",
    "    for model in models:\n",
    "        model.share_memory()\n",
    "    processes = []\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        p = tmp.Process(target=train_slice, \n",
    "                        args=(models[i], dct_x[i, ...], dct_y_cat[i, ...]))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes: \n",
    "        p.join()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading data..\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader = load_mnist_multiprocess(100)\n",
    "\n",
    "for batch_idx, (inputs, labels) in enumerate(trainloader):   \n",
    "    x = inputs\n",
    "    y = labels\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22095e6ad48>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
